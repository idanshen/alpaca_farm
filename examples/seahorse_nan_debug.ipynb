{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21725/2629826844.py:13: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n"
     ]
    }
   ],
   "source": [
    "import t5_encoder\n",
    "import transformers\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer\n",
    "import torch\n",
    "from accelerate.utils import convert_outputs_to_fp32\n",
    "\n",
    "from alpaca_farm import data_utils\n",
    "from argparse import Namespace\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "\n",
    "data_args = Namespace()\n",
    "data_args.prompt_dict_path = pathlib.Path('./prompts/v0_inputs_noinputs.json')\n",
    "data_args.dataset_path = '../seahorse_data/'\n",
    "data_args.classification_label_key = 'question4'\n",
    "\n",
    "training_args = Namespace()\n",
    "training_args.end_sequence_with_eos = False\n",
    "training_args.reward_model_name_or_path = '/mnt/nfs_csail/models/swhan/alpaca_farm/q_four_flant5/'\n",
    "training_args.transformer_cache_dir = None\n",
    "training_args.flash_attn = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_with_native_amp(func, mixed_precision):\n",
    "    \"\"\"Almost like how huggingface accelerate cast `model.forward`.\"\"\"\n",
    "    if mixed_precision not in (\"fp16\", \"bf16\"):\n",
    "        logger.warning(f\"Unknown mixed precision mode: {mixed_precision}, falling back to fp32.\")\n",
    "        return func\n",
    "\n",
    "    if mixed_precision == \"fp16\":\n",
    "        output_func = torch.cuda.amp.autocast(dtype=torch.float16)(func)\n",
    "    else:\n",
    "        device_type = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        output_func = torch.autocast(device_type=device_type, dtype=torch.bfloat16)(func)\n",
    "    output_func = convert_outputs_to_fp32(output_func)\n",
    "    return output_func\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing reward model that is not lora based\n",
      "loading base model /mnt/nfs_csail/models/swhan/alpaca_farm/q_four_flant5/...\n",
      "Loading tokenizer from /mnt/nfs_csail/models/swhan/alpaca_farm/q_four_flant5/\n"
     ]
    }
   ],
   "source": [
    "from alpaca_farm.models.make_models import make_reward_model\n",
    "from alpaca_farm.rl.trainer_utils import _make_padded_tokenizer\n",
    "from alpaca_farm import accelerate_patch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "accelerator = accelerate_patch.MyAccelerator(\n",
    "    gradient_accumulation_steps=1,\n",
    "    mixed_precision='fp16',\n",
    "    even_batches=True,  # Make sure the batch size on each device is the same.\n",
    "    split_batches=False,  # Don't break a batch into smaller chunks.\n",
    "    step_scheduler_with_optimizer=False,  # Untie optimizer and scheduler step.\n",
    "    # Value model might not use all parameters (e.g., lm-head) in the forward pass.\n",
    ")\n",
    "model = make_reward_model(training_args, accelerator, is_trainable=False)\n",
    "tokenizer = _make_padded_tokenizer(training_args.reward_model_name_or_path, cache_dir=None, use_fast_tokenizer=False)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('/mnt/nfs_csail/models/swhan/alpaca_farm/q_four_flant5/')\n",
    "model.forward = cast_with_native_amp(model.forward, 'bf16')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('google/flan-t5-large', model_max_length=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    metrics = metric.compute(predictions=predictions, references=labels)\n",
    "    if np.all(predictions==0) or np.all(predictions==1):\n",
    "        metrics['pearson'] = 0\n",
    "    else:\n",
    "        metrics['pearson'] = np.corrcoef(labels.squeeze(), predictions)[0,1]\n",
    "    print(metrics)\n",
    "    return metrics\n",
    "\n",
    "def format_prompt(example: dict, prompt_dict: dict) -> str:\n",
    "    \"\"\"Formats a prompt with a prompt_dict formatter.\n",
    "\n",
    "    Args:\n",
    "        example: A dict-like object with required keys \"instruction\" and \"input\"\n",
    "        prompt_dict: Dictionary containing the keys \"prompt_noinputs\" and \"prompt_inputs\" which have\n",
    "            placeholders corresponding to the keys from `example`. E.g. \"{instruction}\".\n",
    "\n",
    "    Returns:\n",
    "        A formatted prompt string.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> format_prompt(dict(instruction=\"test\", input=\"\"), prompt_dict=dict(prompt_noinputs=\"prompt {instruction} \"))\n",
    "    \"prompt test\"\n",
    "    \"\"\"\n",
    "    assert \"instruction\" in example and \"input\" in example, \"Internal error: example missing required keys.\"\n",
    "\n",
    "    if example[\"input\"] is None or len(example[\"input\"]) == 0:\n",
    "        formatted_prompt = prompt_dict[\"prompt_noinputs\"].format_map(example)\n",
    "    else:\n",
    "        formatted_prompt = prompt_dict[\"prompt_inputs\"].format_map(example)\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "\n",
    "def format_output_word_by_word(example: dict, eos_token: Optional[str] = None, output_key=\"output\") -> str:\n",
    "    if eos_token is None:\n",
    "        eos_token = \"\"\n",
    "    output = f\"{example[output_key]}{eos_token}\"\n",
    "    return output.split()\n",
    "\n",
    "def format_output(example: dict, eos_token: Optional[str] = None, output_key=\"output\") -> str:\n",
    "    if eos_token is None:\n",
    "        eos_token = \"\"\n",
    "    output = f\"{example[output_key]}{eos_token}\"\n",
    "    return output\n",
    "\n",
    "def _get_text(example: dict, output_key: str):\n",
    "    example['instruction'] = INSTRUCTIONS['seahorse_data']\n",
    "    example['input'] = example['text']\n",
    "    source = format_prompt(example, prompt_dict=prompt_dict)\n",
    "    target = format_output(\n",
    "        example,\n",
    "        eos_token=tokenizer.eos_token if training_args.end_sequence_with_eos else None,\n",
    "        output_key=output_key,\n",
    "    )\n",
    "    return source + ' ' + target\n",
    "\n",
    "def _get_text_target_word_by_word(example: dict, output_key: str):\n",
    "    example['instruction'] = INSTRUCTIONS['seahorse_data']\n",
    "    example['input'] = example['text']\n",
    "    source = format_prompt(example, prompt_dict=prompt_dict)\n",
    "    target = format_output_word_by_word(\n",
    "        example,\n",
    "        eos_token=tokenizer.eos_token if training_args.end_sequence_with_eos else None,\n",
    "        output_key=output_key,\n",
    "    )\n",
    "    return [source + ' ' + ' '.join(target[:t]) for t in range(len(target))], target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_module = data_utils.make_classification_reward_modeling_data_module(\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_args=data_args,\n",
    "#         training_args=training_args,\n",
    "#     )\n",
    "import datasets\n",
    "from alpaca_farm import utils\n",
    "\n",
    "prompt_dict = utils.jload(data_args.prompt_dict_path)\n",
    "data_files = {\"train\": \"train.json\", \"validation\": \"validation.json\"}\n",
    "dataset_json = datasets.load_dataset(data_args.dataset_path, data_files=data_files)\n",
    "dataset_json = dataset_json.filter(lambda example: example['worker_lang'] == 'en-US')\n",
    "train_dataset = dataset_json['train']\n",
    "eval_dataset = dataset_json['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "INSTRUCTIONS = {\n",
    "    'seahorse_data': \"Generate a one-sentence summary of this post.\",\n",
    "}\n",
    "eval_dict_data = pd.DataFrame(eval_dataset).to_dict(orient=\"records\")\n",
    "\n",
    "indices_to_remove = []\n",
    "for i, dict_data in enumerate(eval_dict_data):\n",
    "    if dict_data['text'] is None:\n",
    "        indices_to_remove.append(i)\n",
    "for index in sorted(indices_to_remove, reverse=True):\n",
    "    del eval_dict_data[index]\n",
    "\n",
    "print(len(indices_to_remove))\n",
    "\n",
    "train_dict_data = pd.DataFrame(train_dataset).to_dict(orient=\"records\")\n",
    "\n",
    "indices_to_remove = []\n",
    "for i, dict_data in enumerate(train_dict_data):\n",
    "    if dict_data['text'] is None:\n",
    "        indices_to_remove.append(i)\n",
    "for index in sorted(indices_to_remove, reverse=True):\n",
    "    del train_dict_data[index]\n",
    "\n",
    "print(len(indices_to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|‚ñç                                                                                                                                                    | 7/2183 [00:00<02:13, 16.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                                           | 109/2183 [00:03<00:51, 40.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7227722772277227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                                                                                                                     | 209/2183 [00:09<01:02, 31.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7313432835820896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                                              | 311/2183 [00:14<01:45, 17.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7475083056478405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                                                                                       | 409/2183 [00:17<00:41, 42.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7456359102244389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                                                                                                                | 509/2183 [00:23<00:50, 33.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7385229540918163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                          | 608/2183 [00:28<01:54, 13.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7371048252911814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                                                   | 711/2183 [00:31<00:30, 47.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7275320970042796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                                                                            | 808/2183 [00:36<00:44, 31.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7228464419475655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                                     | 908/2183 [00:42<01:24, 15.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7125416204217536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                               | 1000/2183 [00:45<00:26, 44.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7072927072927073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                                                       | 1109/2183 [00:50<00:29, 36.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7057220708446866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                                                                       | 1121/2183 [00:51<00:26, 40.57it/s]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import trange\n",
    "num_correct = 0.\n",
    "num_total = 0.\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in trange(len(eval_dataset)):\n",
    "        sequences = _get_text(eval_dict_data[i], 'summary')\n",
    "        input_ids = tokenizer(sequences, truncation=True, return_tensors='pt')['input_ids']\n",
    "        # with torch.cuda.amp.autocast(enabled=True, dtype=torch.bfloat16):        \n",
    "        outputs = model(input_ids.cuda())\n",
    "        rewards = outputs.rewards\n",
    "        label = eval_dict_data[i]['question4']\n",
    "        pred = 1. if rewards > 0 else 0.\n",
    "        if pred == label:\n",
    "            num_correct +=1\n",
    "        num_total +=1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(num_correct/num_total)\n",
    "        if torch.isnan(rewards).any() or torch.isinf(rewards).any():\n",
    "            print(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
