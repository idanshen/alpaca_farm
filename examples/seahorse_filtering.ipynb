{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc58962-7e2f-4f30-a5fb-a2ba289b1a87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 40201.00it/s]\n",
      "Extracting data files: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 42.69it/s]\n",
      "Generating train split: 60980 examples [00:00, 326666.37 examples/s]\n",
      "Generating test split: 18331 examples [00:00, 141846.27 examples/s]\n",
      "Generating validation split: 8968 examples [00:00, 148713.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import tensorflow_datasets as tfds\n",
    "from alpaca_farm.utils import jdump\n",
    "\n",
    "dataset_path = \"/data/pulkitag/models/idanshen/alpaca_farm/seahorse_data/\"\n",
    "data_files = {\"train\": \"train.tsv\", \"test\": \"test.tsv\", \"validation\": \"validation.tsv\"}\n",
    "dataset = load_dataset(dataset_path, data_files=data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "42defe98-4ea8-434c-b8bb-c18e2f5803d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = [f for f in iter(dataset['train'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "eb63c4f7-1832-4968-ae15-19f1b98398d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds, info = tfds.load(f'huggingface:gem/wiki_lingua_english_en', split='validation', with_info=True)\n",
    "hfdf_english_en = tfds.as_dataframe(ds,info)\n",
    "data_wiki_lingua_english_en = {id: content for id, content in zip([f for f in hfdf_english_en['gem_id']], [f for f in hfdf_english_en['source_aligned/en']])}\n",
    "ds, info = tfds.load(f'huggingface:gem/wiki_lingua_german_de', split='validation', with_info=True)\n",
    "hfdf_german_de = tfds.as_dataframe(ds,info)\n",
    "data_wiki_lingua_german_de = {id: content for id, content in zip([f for f in hfdf_german_de['gem_id']], [f for f in hfdf_german_de['source_aligned/de']])}\n",
    "ds, info = tfds.load(f'huggingface:gem/wiki_lingua_russian_ru', split='validation', with_info=True)\n",
    "hfdf_russian_ru = tfds.as_dataframe(ds,info)\n",
    "data_wiki_lingua_russian_ru = {id: content for id, content in zip([f for f in hfdf_russian_ru['gem_id']], [f for f in hfdf_russian_ru['source_aligned/ru']])}\n",
    "ds, info = tfds.load(f'huggingface:gem/wiki_lingua_vietnamese_vi', split='validation', with_info=True)\n",
    "hfdf_vietnamese_vi = tfds.as_dataframe(ds,info)\n",
    "data_wiki_lingua_vietnamese_vi = {id: content for id, content in zip([f for f in hfdf_vietnamese_vi['gem_id']], [f for f in hfdf_vietnamese_vi['source_aligned/vi']])}\n",
    "ds, info = tfds.load(f'huggingface:gem/wiki_lingua_turkish_tr', split='validation', with_info=True)\n",
    "hfdf_turkish_tr = tfds.as_dataframe(ds,info)\n",
    "data_wiki_lingua_turkish_tr = {id: content for id, content in zip([f for f in hfdf_turkish_tr['gem_id']], [f for f in hfdf_turkish_tr['source_aligned/tr']])}\n",
    "ds, info = tfds.load(f'huggingface:gem/wiki_lingua_spanish_es', split='validation', with_info=True)\n",
    "hfdf_spanish_es = tfds.as_dataframe(ds,info)\n",
    "data_wiki_lingua_spanish_es = {id: content for id, content in zip([f for f in hfdf_spanish_es['gem_id']], [f for f in hfdf_spanish_es['source_aligned/es']])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2fca65d4-08bd-4800-bf2e-5f92a73a0df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xsum_validation = load_dataset('GEM/xsum')['validation']\n",
    "data_mlsum_es_validation = load_dataset('GEM/mlsum', 'es')['validation']\n",
    "data_mlsum_de_validation = load_dataset('GEM/mlsum', 'de')['validation']\n",
    "data_xlsum_english_validation = load_dataset('GEM/xlsum', 'english')['validation']\n",
    "data_xlsum_spanish_validation = load_dataset('GEM/xlsum', 'spanish')['validation']\n",
    "data_xlsum_russian_validation = load_dataset('GEM/xlsum', 'russian')['validation']\n",
    "data_xlsum_vietnamese_validation = load_dataset('GEM/xlsum', 'vietnamese')['validation']\n",
    "data_xlsum_turkish_validation = load_dataset('GEM/xlsum', 'turkish')['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "3de30550-5982-4c93-a669-dca5f98d3106",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "found = 0\n",
    "data = []\n",
    "for i, line in enumerate(t):\n",
    "    line['question1'] = 1 if line['question1'] == 'Yes' else 0\n",
    "    line['question2'] = 1 if line['question2'] == 'Yes' else 0\n",
    "    line['question3'] = 1 if line['question3'] == 'Yes' else 0\n",
    "    line['question4'] = 1 if line['question4'] == 'Yes' else 0\n",
    "    line['question5'] = 1 if line['question5'] == 'Yes' else 0\n",
    "    line['question6'] = 1 if line['question6'] == 'Yes' else 0\n",
    "    if line['gem_id'] is None:\n",
    "        continue\n",
    "    elif 'xsum-validation' in line['gem_id']:\n",
    "        line['text'] = data_xsum_validation.select([data_xsum_validation['gem_id'].index(line['gem_id'])])['document'][0]\n",
    "        data.append(line)\n",
    "    elif 'mlsum_es-validation' in line['gem_id']:\n",
    "        line['text'] = data_mlsum_es_validation.select([data_mlsum_es_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "        data.append(line)\n",
    "    # elif 'mlsum_de-validation' in line['gem_id']:\n",
    "    #     line['text'] = data_mlsum_de_validation.select([data_mlsum_de_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "    elif 'xlsum_english-validation' in line['gem_id']:\n",
    "        line['text'] = data_xlsum_english_validation.select([data_xlsum_english_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "        data.append(line)\n",
    "    # elif 'xlsum_spanish-validation' in line['gem_id']:\n",
    "    #     line['text'] = data_xlsum_spanish_validation.select([data_xlsum_spanish_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "    # elif 'xlsum_russian-validation' in line['gem_id']:\n",
    "    #     line['text'] = data_xlsum_russian_validation.select([data_xlsum_russian_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "    # elif 'xlsum_vietnamese-validation' in line['gem_id']:\n",
    "    #     line['text'] = data_xlsum_vietnamese_validation.select([data_xlsum_vietnamese_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "    # elif 'xlsum_turkish-validation' in line['gem_id']:\n",
    "    #     line['text'] = data_xlsum_turkish_validation.select([data_xlsum_turkish_validation['gem_id'].index(line['gem_id'])])['text'][0]\n",
    "    elif 'wiki_lingua_english_en-val' in line['gem_id']:\n",
    "        if line['gem_id'].encode('UTF-8') in data_wiki_lingua_english_en:\n",
    "            line['text'] = data_wiki_lingua_english_en[line['gem_id'].encode('UTF-8')]\n",
    "            data.append(line)\n",
    "    # elif 'wiki_lingua_german_de-val' in line['gem_id']:\n",
    "    #     if line['gem_id'].encode('UTF-8') in data_wiki_lingua_german_de:\n",
    "    #         line['text'] = data_wiki_lingua_german_de[line['gem_id'].encode('UTF-8')]\n",
    "    # elif 'wiki_lingua_russian_ru-val' in line['gem_id']:\n",
    "    #     if line['gem_id'].encode('UTF-8') in data_wiki_lingua_russian_ru:\n",
    "    #         line['text'] = data_wiki_lingua_russian_ru[line['gem_id'].encode('UTF-8')]\n",
    "    # elif 'wiki_lingua_vietnamese_vi-val' in line['gem_id']:\n",
    "    #     if line['gem_id'].encode('UTF-8') in data_wiki_lingua_vietnamese_vi:\n",
    "    #         line['text'] = data_wiki_lingua_vietnamese_vi[line['gem_id'].encode('UTF-8')]\n",
    "    # elif 'wiki_lingua_turkish_tr-val' in line['gem_id']:\n",
    "    #     if line['gem_id'].encode('UTF-8') in data_wiki_lingua_turkish_tr:\n",
    "    #         line['text'] = data_wiki_lingua_turkish_tr[line['gem_id'].encode('UTF-8')]\n",
    "    # elif 'wiki_lingua_spanish_es-val' in line['gem_id']:\n",
    "    #     if line['gem_id'].encode('UTF-8') in data_wiki_lingua_spanish_es:\n",
    "    #         line['text'] = data_wiki_lingua_spanish_es[line['gem_id'].encode('UTF-8')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "377fb2db-98c9-407e-bfba-003969302ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17966"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "b356414f-9b25-4e5b-bcd6-e1951448b5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(data):\n",
    "    if 'text' not in line:\n",
    "        print(i)\n",
    "        print('error')\n",
    "    elif line['text'] is None:\n",
    "        print(i)\n",
    "        print('error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "c3dcc176-b84b-4791-ba3f-97341cee9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jdump(t,dataset_path+str(\"validation.json\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
